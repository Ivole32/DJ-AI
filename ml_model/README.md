
# ML Model - DJ Track Transition Prediction

Machine learning module for predicting good track transitions in DJ sets.

## Structure

```
ml_model/
‚îú‚îÄ‚îÄ config.py              # Configuration
‚îú‚îÄ‚îÄ data_preparation/      # Data extraction/preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ export_transitions.py
‚îú‚îÄ‚îÄ training/              # Model training
‚îÇ   ‚îú‚îÄ‚îÄ training_data.py
‚îÇ   ‚îî‚îÄ‚îÄ train.py
‚îú‚îÄ‚îÄ inference/             # Prediction/validation
‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îî‚îÄ‚îÄ validation.py
‚îú‚îÄ‚îÄ utils/                 # Shared utilities
‚îÇ   ‚îî‚îÄ‚îÄ feature_builder.py
‚îú‚îÄ‚îÄ models/                # Trained model files
‚îÇ   ‚îî‚îÄ‚îÄ transition_model.joblib
‚îî‚îÄ‚îÄ requirements.txt       # Python dependencies
```

## Quick Start

### 1. Install Dependencies
```bash
cd ml_model
pip install -r requirements.txt
```

### 2. Prepare Training Data
```bash
python data_preparation/export_transitions.py
```
Creates `dataset/transitions.csv` with all track-to-track transitions from DJ mixes.

### 3. Train the Model
```bash
python training/train.py
```
Requires:
- `dataset/tracks.csv` (analyzed tracks)
- `dataset/transitions.csv` (real DJ transitions)

The model is saved to `models/transition_model.joblib`.

---
For more details, see the main project README.

**Note:** The Camelot Wheel notation (e.g., "8A", "11B") is crucial for the model - it enables harmonic mixing predictions based on DJ key compatibility.

### 4. Validate Setup (Optional but Recommended)

Before making predictions, verify everything is ready:

```bash
python inference/validation.py
```

This checks:
- ‚úì All required files exist (tracks.csv, model)
- ‚úì All dependencies are importable
- ‚úì Paths are correctly configured

**Quick validation in code:**

```python
from ml_model import quick_validate

if quick_validate():
    print("‚úì Model ready!")
else:
    print("‚úó Setup incomplete - check files and dependencies")
```

### 5. Make Predictions

```python
from inference.predict import suggest_next

# Get top 10 suggestions for next track
suggestions = suggest_next('youtube_video_id', top_k=10)

for track_id, score in suggestions:
    print(f"{track_id}: {score:.3f}")
```

## üìä How It Works

### Feature Engineering

The model uses these features to predict good transitions:

- **BPM Difference**: Absolute tempo difference between tracks
- **Energy Difference**: Change in energy level (increase/decrease)
- **Key Distance**: Harmonic distance using Camelot Wheel notation
- **Same Key**: Binary flag for identical keys
- **Tempo Match**: Binary flag for BPM within ¬±2

### Training Data

- **Positive Examples**: Real transitions from DJ mixes (weighted by frequency)
- **Negative Examples**: Random track pairs that likely wouldn't work well
- **Ratio**: 3 negative examples per positive to balance classes

### Model Architecture

**HistGradientBoostingClassifier** with:
- Max depth: 6 (prevents overfitting)
- Iterations: 200
- Learning rate: 0.05 (stable convergence)
- Min samples per leaf: 20 (reduces noise)
- L2 regularization: 1.0
- Early stopping enabled

## üìù Module Documentation

### config.py

**Centralized configuration** for all ML model modules:

Uses `pathlib` for cross-platform paths. Automatically creates required directories.

**Key settings:**
- `TRACKS_CSV`: Path to analyzed tracks (dataset/tracks.csv)
- `TRANSITIONS_CSV`: Path to extracted transitions (dataset/transitions.csv)
- `DATASET_JSON`: Path to DJ mix dataset (dataset/dataset.json)
- `MODEL_FILE`: Path to trained model (models/transition_model.joblib)
- `MODELS_DIR`: Directory for model files
- Training hyperparameters (MAX_DEPTH, MAX_ITER, LEARNING_RATE, etc.)
- Training data generation parameters (NEGATIVES_PER_POS, MAX_FREQUENCY_WEIGHT)

All modules import paths and parameters from this central config for consistency.

### data_preparation/

**export_transitions.py**: Extracts track-to-track transitions from `dataset.json`
- Counts frequency of each transition
- Exports to CSV format
- Filters out tracks with null IDs
- Uses centralized config for paths

### training/

**training_data.py**: Generates labeled training set
- `build_training_set()`: Creates feature matrix and labels
- Balances positive/negative examples
- Weights frequent transitions using `MAX_FREQUENCY_WEIGHT` from config
- Uses `NEGATIVES_PER_POS` from config for class balance

**train.py**: Main training script
- Loads and preprocesses data
- Trains the model using hyperparameters from config
- Saves trained model to disk

### inference/

**predict.py**: Prediction API
- `TrackPredictor`: Class-based predictor with full control
- `suggest_next()`: Convenience function for quick predictions
- Returns ranked list of (track_id, probability) tuples

**validation.py**: Model validation utilities for setup verification

This module provides three levels of validation to ensure the model is ready:

**1. `validate_model_files(tracks_path=None, model_path=None)`**
- Checks file existence: `tracks.csv` and `transition_model.joblib`
- Returns detailed dict with `ok` (bool), `missing` (list), and file paths
- Use before training or prediction to catch missing data early

```python
from ml_model import validate_model_files

result = validate_model_files()
if result['ok']:
    print(f"‚úì Files ready: {result['tracks_path']}")
else:
    print(f"‚úó Missing: {result['missing']}")
```

**2. `validate_imports()`**
- Tests if all required packages can be imported
- Checks: pandas, joblib, sklearn, and local utils
- Returns dict with `ok` (bool) and `missing` (list)
- Use to diagnose dependency issues

```python
from ml_model import validate_imports

result = validate_imports()
if not result['ok']:
    print(f"Install missing packages: {result['missing']}")
```

**3. `quick_validate(tracks_path=None, model_path=None)`**
- Fast boolean check: returns `True` if everything is ready
- Combines file and import validation
- Ideal for quick checks in production code

```python
from ml_model import quick_validate

if not quick_validate():
    raise RuntimeError("ML model not ready - run validation.py for details")
```

**Standalone Diagnostic Tool:**

Run as a script for a complete system check:

```bash
python inference/validation.py
```

Output example:
```
============================================================
ML Model Validation
============================================================

1. Checking files...
   ‚úì All files exist
   Tracks: C:\...\analyzer\output\tracks.csv
   Model: C:\...\ml_model\models\transition_model.joblib

2. Checking imports...
   ‚úì All imports available

============================================================
Ready: True
============================================================
```

### utils/

**feature_builder.py**: Feature engineering functions
- `camelot_distance()`: Calculates harmonic key distance
- `build_pair_features()`: Extracts features from track pairs

## üîÑ Workflow

```
dataset.json ‚Üí export_transitions.py ‚Üí transitions.csv
                                              ‚Üì
tracks.csv ‚Üí training_data.py ‚Üí feature matrix
                                              ‚Üì
                                       train.py ‚Üí transition_model.joblib
                                                            ‚Üì
                                                      predict.py ‚Üí suggestions
```

## üõ†Ô∏è Configuration

All configuration is centralized in `config.py` for easy customization.

### Model Hyperparameters

Edit `config.py` to adjust model training parameters:
- `MAX_DEPTH`: Tree depth (controls complexity) - Default: 6
- `MAX_ITER`: Number of boosting rounds - Default: 200
- `LEARNING_RATE`: Step size for gradient descent - Default: 0.05
- `MIN_SAMPLES_LEAF`: Minimum samples per leaf node - Default: 20
- `L2_REGULARIZATION`: L2 penalty for regularization - Default: 1.0
- `EARLY_STOPPING`: Stop if validation doesn't improve - Default: True
- `RANDOM_STATE`: Reproducibility seed - Default: 42

### Training Data Balance

Edit `config.py` to adjust training data generation:
- `NEGATIVES_PER_POS`: Ratio of negative to positive examples - Default: 3
- `MAX_FREQUENCY_WEIGHT`: Cap on repeating frequent transitions - Default: 3

## üìà Performance Tips

1. **More Data**: Analyze more DJ mixes to get more transitions
2. **Feature Engineering**: Add new features (genre, mood, decade, etc.)
3. **Hyperparameter Tuning**: Use GridSearchCV for optimal parameters
4. **Class Weighting**: Adjust if precision/recall is imbalanced

## üêõ Troubleshooting

**FileNotFoundError**: Ensure you've run the analyzer and have `tracks.csv`
- Run `python inference/validation.py` to see which files are missing
- Check paths: `dataset/tracks.csv` and `models/transition_model.joblib`

**ImportError**: Missing dependencies
- Run `python inference/validation.py` to identify missing packages
- Install with: `pip install -r requirements.txt`

**KeyError in predict**: Track ID not in dataset
- Track hasn't been analyzed yet - add it to analyzer queue

**Poor predictions**: Need more training data or better feature engineering
- Analyze more DJ mixes to increase training transitions
- Check if `transitions.csv` has sufficient variety

## üìö Dependencies

- pandas: Data manipulation
- numpy: Numerical operations
- scikit-learn: Machine learning
- joblib: Model serialization

## ü§ù Integration

This model is used by:
- `api/`: REST API endpoints for track suggestions
- `frontend/`: Web interface for DJ mix planning

## ‚ö†Ô∏è Python Version

**Recommended Python Version: 3.11.9**

Some dependencies in this project require Python 3.11.9 for optimal compatibility. This version has been tested and is recommended for use with all libraries.